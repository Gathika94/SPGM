{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd90de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import xlwt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from src.dataset.data_loader import GMDataset, get_dataloader\n",
    "from src.displacement_layer import Displacement\n",
    "from src.loss_func import *\n",
    "from src.evaluation_metric import matching_accuracy\n",
    "from src.parallel import DataParallel\n",
    "from src.utils.model_sl import load_model, save_model\n",
    "from eval import eval_model\n",
    "from src.lap_solvers.hungarian import hungarian\n",
    "from src.utils.data_to_cuda import data_to_cuda\n",
    "\n",
    "from src.utils.config import cfg\n",
    "from pygmtools.benchmark import Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(model,\n",
    "                     criterion,\n",
    "                     optimizer,\n",
    "                     optimizer_k,\n",
    "                     image_dataset,\n",
    "                     dataloader,\n",
    "                     tfboard_writer,\n",
    "                     benchmark,\n",
    "                     num_epochs=25,\n",
    "                     start_epoch=0,\n",
    "                     xls_wb=None):\n",
    "    print('Start training...')\n",
    "\n",
    "    since = time.time()\n",
    "    dataset_size = len(dataloader['train'].dataset)\n",
    "    lambda_value = 0.4\n",
    "    displacement = Displacement()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    print('model on device: {}'.format(device))\n",
    "\n",
    "    checkpoint_path = Path(cfg.OUTPUT_PATH) / 'params'\n",
    "    if not checkpoint_path.exists():\n",
    "        checkpoint_path.mkdir(parents=True)\n",
    "\n",
    "    model_path, optim_path, optim_k_path = '', '', ''\n",
    "    if start_epoch != 0:\n",
    "        model_path = str(checkpoint_path / 'params_{:04}.pt'.format(start_epoch))\n",
    "        optim_path = str(checkpoint_path / 'optim_{:04}.pt'.format(start_epoch))\n",
    "        if optimizer_k is not None:\n",
    "            optim_k_path = str(checkpoint_path / 'optim_k_{:04}.pt'.format(start_epoch))\n",
    "    if len(cfg.PRETRAINED_PATH) > 0:\n",
    "        model_path = cfg.PRETRAINED_PATH\n",
    "    if len(model_path) > 0:\n",
    "        print('Loading model parameters from {}'.format(model_path))\n",
    "        load_model(model, model_path, strict=False)\n",
    "    if len(optim_path) > 0:\n",
    "        print('Loading optimizer state from {}'.format(optim_path))\n",
    "        optimizer.load_state_dict(torch.load(optim_path))\n",
    "    if len(optim_k_path) > 0:\n",
    "        print('Loading optimizer_k state from {}'.format(optim_k_path))\n",
    "        optimizer_k.load_state_dict(torch.load(optim_k_path))\n",
    "\n",
    "    if optimizer_k is not None:\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                   milestones=cfg.TRAIN.LR_STEP,\n",
    "                                                   gamma=cfg.TRAIN.LR_DECAY,\n",
    "                                                   last_epoch=-1)  # cfg.TRAIN.START_EPOCH - 1\n",
    "        scheduler_k = optim.lr_scheduler.MultiStepLR(optimizer_k,\n",
    "                                                   milestones=cfg.TRAIN.LR_STEP,\n",
    "                                                   gamma=cfg.TRAIN.LR_DECAY,\n",
    "                                                   last_epoch=-1)  # cfg.TRAIN.START_EPOCH - 1\n",
    "    else:\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                   milestones=cfg.TRAIN.LR_STEP,\n",
    "                                                   gamma=cfg.TRAIN.LR_DECAY,\n",
    "                                                   last_epoch=cfg.TRAIN.START_EPOCH - 1)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Reset seed after evaluation per epoch\n",
    "        torch.manual_seed(cfg.RANDOM_SEED + epoch + 1)\n",
    "        dataloader['train'] = get_dataloader(image_dataset['train'], shuffle=True, fix_seed=False)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        model.module.trainings = True\n",
    "\n",
    "        print('lr = ' + ', '.join(['{:.2e}'.format(x['lr']) for x in optimizer.param_groups]))\n",
    "        if optimizer_k is not None:\n",
    "            print('K_regression_lr = ' + ', '.join(['{:.2e}'.format(x['lr']) for x in optimizer_k.param_groups]))\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        running_ks_loss = 0.0\n",
    "        running_ks_error = 0\n",
    "        running_since = time.time()\n",
    "        iter_num = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs in dataloader['train']:\n",
    "            if iter_num >= cfg.TRAIN.EPOCH_ITERS:\n",
    "                break\n",
    "            if model.module.device != torch.device('cpu'):\n",
    "                inputs = data_to_cuda(inputs)\n",
    "\n",
    "            iter_num = iter_num + 1\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            if optimizer_k is not None:\n",
    "                optimizer_k.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # torch.autograd.set_detect_anomaly(True)\n",
    "                # forward\n",
    "                if 'common' in cfg.MODEL_NAME: # COMMON use the iter number to control the warmup temperature\n",
    "                    outputs = model(inputs, training=True, iter_num=iter_num, epoch=epoch)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                if cfg.PROBLEM.TYPE == '2GM':\n",
    "                    assert 'ds_mat' in outputs\n",
    "                    assert 'perm_mat' in outputs\n",
    "                    assert 'gt_perm_mat' in outputs\n",
    "\n",
    "                  \n",
    "                    if cfg.TRAIN.LOSS_FUNC == 'pml':\n",
    "                        loss = criterion(outputs['ds_mat'], outputs['gt_perm_mat'], outputs['perm_mat'],outputs['alpha_f'],outputs['beta_f'],lambda_value,*outputs['ns'])\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            'Unsupported loss function {} for problem type {}'.format(cfg.TRAIN.LOSS_FUNC,\n",
    "                                                                                      cfg.PROBLEM.TYPE))\n",
    "                    if 'ks_loss' in outputs:\n",
    "                        ks_loss = outputs['ks_loss']\n",
    "                        ks_error = outputs['ks_error']\n",
    "\n",
    "                    # compute accuracy\n",
    "                    acc = matching_accuracy(outputs['perm_mat'], outputs['gt_perm_mat'], outputs['ns'], idx=0)\n",
    "\n",
    "                elif cfg.PROBLEM.TYPE in ['MGM', 'MGM3']:\n",
    "                    assert 'ds_mat_list' in outputs\n",
    "                    assert 'graph_indices' in outputs\n",
    "                    assert 'perm_mat_list' in outputs\n",
    "                    if not 'gt_perm_mat_list' in outputs:\n",
    "                        assert 'gt_perm_mat' in outputs\n",
    "                        gt_perm_mat_list = [outputs['gt_perm_mat'][idx] for idx in outputs['graph_indices']]\n",
    "                    else:\n",
    "                        gt_perm_mat_list = outputs['gt_perm_mat_list']\n",
    "\n",
    "                    # compute loss & accuracy\n",
    "                    if cfg.TRAIN.LOSS_FUNC in ['perm', 'ce' 'hung']:\n",
    "                        loss = torch.zeros(1, device=model.module.device)\n",
    "                        ns = outputs['ns']\n",
    "                        for s_pred, x_gt, (idx_src, idx_tgt) in \\\n",
    "                                zip(outputs['ds_mat_list'], gt_perm_mat_list, outputs['graph_indices']):\n",
    "                            l = criterion(s_pred, x_gt, ns[idx_src], ns[idx_tgt])\n",
    "                            loss += l\n",
    "                        loss /= len(outputs['ds_mat_list'])\n",
    "                    elif cfg.TRAIN.LOSS_FUNC == 'plain':\n",
    "                        loss = torch.sum(outputs['loss'])\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            'Unsupported loss function {} for problem type {}'.format(cfg.TRAIN.LOSS_FUNC,\n",
    "                                                                                      cfg.PROBLEM.TYPE))\n",
    "\n",
    "                    # compute accuracy\n",
    "                    acc = torch.zeros(1, device=model.module.device)\n",
    "                    for x_pred, x_gt, (idx_src, idx_tgt) in \\\n",
    "                            zip(outputs['perm_mat_list'], gt_perm_mat_list, outputs['graph_indices']):\n",
    "                        a = matching_accuracy(x_pred, x_gt, ns, idx=idx_src)\n",
    "                        acc += torch.sum(a)\n",
    "                    acc /= len(outputs['perm_mat_list'])\n",
    "                else:\n",
    "                    raise ValueError('Unknown problem type {}'.format(cfg.PROBLEM.TYPE))\n",
    "\n",
    "                # backward + optimize\n",
    "                if cfg.FP16:\n",
    "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    # with torch.autograd.detect_anomaly():\n",
    "                    loss.backward()\n",
    "                    if optimizer_k is not None:\n",
    "                        ks_loss.backward()\n",
    "                # for n, p in model.named_parameters():\n",
    "                #     if p.grad is not None and torch.any(torch.isnan(p.grad)):\n",
    "                #         print('NaN!!!')\n",
    "                #         print('name:', n, '-->require_grad:', p.requires_grad)\n",
    "                optimizer.step()\n",
    "                if optimizer_k is not None:\n",
    "                    optimizer_k.step()\n",
    "\n",
    "                batch_num = inputs['batch_size']\n",
    "\n",
    "                # tfboard writer\n",
    "                loss_dict = dict()\n",
    "                loss_dict['loss'] = loss.item()\n",
    "                tfboard_writer.add_scalars('loss', loss_dict, epoch * cfg.TRAIN.EPOCH_ITERS + iter_num)\n",
    "\n",
    "                accdict = dict()\n",
    "                accdict['matching accuracy'] = torch.mean(acc)\n",
    "                tfboard_writer.add_scalars(\n",
    "                    'training accuracy',\n",
    "                    accdict,\n",
    "                    epoch * cfg.TRAIN.EPOCH_ITERS + iter_num\n",
    "                )\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * batch_num\n",
    "                epoch_loss += loss.item() * batch_num\n",
    "                if 'ks_loss' in outputs:\n",
    "                    running_ks_loss += ks_loss * batch_num\n",
    "                    running_ks_error += ks_error * batch_num\n",
    "\n",
    "                if iter_num % cfg.STATISTIC_STEP == 0:\n",
    "                    running_speed = cfg.STATISTIC_STEP * batch_num / (time.time() - running_since)\n",
    "                    print('Epoch {:<4} Iteration {:<4} {:>4.2f}sample/s Loss={:<8.4f} '\n",
    "                          .format(epoch, iter_num, running_speed, running_loss / cfg.STATISTIC_STEP / batch_num))\n",
    "                    tfboard_writer.add_scalars(\n",
    "                        'speed',\n",
    "                        {'speed': running_speed},\n",
    "                        epoch * cfg.TRAIN.EPOCH_ITERS + iter_num\n",
    "                    )\n",
    "\n",
    "                    tfboard_writer.add_scalars(\n",
    "                        'learning rate',\n",
    "                        {'lr_{}'.format(i): x['lr'] for i, x in enumerate(optimizer.param_groups)},\n",
    "                        epoch * cfg.TRAIN.EPOCH_ITERS + iter_num\n",
    "                    )\n",
    "\n",
    "                    running_loss = 0.0\n",
    "                    running_ks_loss = 0.0\n",
    "                    running_ks_error = 0.0\n",
    "                    running_since = time.time()\n",
    "\n",
    "        epoch_loss = epoch_loss / cfg.TRAIN.EPOCH_ITERS / batch_num\n",
    "\n",
    "        save_model(model, str(checkpoint_path / 'params_{:04}.pt'.format(epoch + 1)))\n",
    "        torch.save(optimizer.state_dict(), str(checkpoint_path / 'optim_{:04}.pt'.format(epoch + 1)))\n",
    "        if optimizer_k is not None:\n",
    "            torch.save(optimizer_k.state_dict(), str(checkpoint_path / 'optim_k_{:04}.pt'.format(epoch + 1)))\n",
    "\n",
    "        print('Epoch {:<4} Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "        print()\n",
    "\n",
    "        # Eval in each epoch\n",
    "        if(epoch >= 0):\n",
    "            if dataloader['test'].dataset.cls not in ['none', 'all', None]:\n",
    "                clss = [dataloader['test'].dataset.cls]\n",
    "            else:\n",
    "                clss = dataloader['test'].dataset.bm.classes\n",
    "            l_e = (epoch == (num_epochs - 1))\n",
    "            accs = eval_model(model, clss, benchmark['test'], l_e,\n",
    "                              xls_sheet=xls_wb.add_sheet('epoch{}'.format(epoch + 1)))\n",
    "            acc_dict = {\"{}\".format(cls): single_acc for cls, single_acc in zip(dataloader['test'].dataset.classes, accs)}\n",
    "            acc_dict['average'] = torch.mean(accs)\n",
    "            tfboard_writer.add_scalars(\n",
    "                'Eval acc',\n",
    "                acc_dict,\n",
    "                (epoch + 1) * cfg.TRAIN.EPOCH_ITERS\n",
    "            )\n",
    "            wb.save(wb.__save_path)\n",
    "\n",
    "        scheduler.step()\n",
    "        if optimizer_k is not None:\n",
    "            scheduler_k.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'\n",
    "          .format(time_elapsed // 3600, (time_elapsed // 60) % 60, time_elapsed % 60))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dup_stdout_manager import DupStdoutFileManager\n",
    "from src.utils.parse_args import parse_args\n",
    "from src.utils.print_easydict import print_easydict   \n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16526789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from src.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "f='experiments/vgg16_gcan_spgm2_imcpt_100.yaml'\n",
    "cfg_from_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cfg.MODEL_NAME) != 0 and len(cfg.DATASET_NAME) != 0:\n",
    "        outp_path = get_output_dir(cfg.MODEL_NAME, cfg.DATASET_NAME)\n",
    "        print(cfg.MODEL_NAME)\n",
    "        cfg_from_list(['OUTPUT_PATH', outp_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e84a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(cfg.OUTPUT_PATH).exists():\n",
    "        Path(cfg.OUTPUT_PATH).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43569988",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = importlib.import_module(cfg.MODULE)\n",
    "Net = mod.Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5739161",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(cfg.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = {'train': cfg.TRAIN.EPOCH_ITERS * cfg.BATCH_SIZE, 'test': cfg.EVAL.SAMPLES}\n",
    "ds_dict = cfg[cfg.DATASET_FULL_NAME] if ('DATASET_FULL_NAME' in cfg) and (cfg.DATASET_FULL_NAME in cfg) else {}\n",
    "benchmark = {\n",
    "        x: Benchmark(name=cfg.DATASET_FULL_NAME,\n",
    "                     sets=x,\n",
    "                     problem=cfg.PROBLEM.TYPE,\n",
    "                     obj_resize=cfg.PROBLEM.RESCALE,\n",
    "                     filter=cfg.PROBLEM.FILTER,\n",
    "                     **ds_dict)\n",
    "        for x in ('train', 'test')}\n",
    "\n",
    "image_dataset = {\n",
    "        x: GMDataset(cfg.DATASET_FULL_NAME,\n",
    "                     benchmark[x],\n",
    "                     dataset_len[x],\n",
    "                     cfg.PROBLEM.TRAIN_ALL_GRAPHS if x == 'train' else cfg.PROBLEM.TEST_ALL_GRAPHS,\n",
    "                     cfg.TRAIN.CLASS if x == 'train' else cfg.EVAL.CLASS,\n",
    "                     cfg.PROBLEM.TYPE)\n",
    "        for x in ('train', 'test')}\n",
    "dataloader = {x: get_dataloader(image_dataset[x], shuffle=True, fix_seed=(x == 'test'))\n",
    "                  for x in ('train', 'test')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b623a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.TRAIN.LOSS_FUNC.lower() == 'pml':\n",
    "    criterion = PMLoss()\n",
    "\n",
    "    print('NOTE: You are setting the loss function as \\'custom\\', please ensure that there is a tensor with key '\n",
    "              '\\'loss\\' in your model\\'s returned dictionary.')\n",
    "else:\n",
    "    raise ValueError('Unknown loss function {}'.format(cfg.TRAIN.LOSS_FUNC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66da96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_k = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64196086",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.TRAIN.SEPARATE_BACKBONE_LR:\n",
    "        if not cfg.TRAIN.SEPARATE_K_LR:\n",
    "            backbone_ids = [id(item) for item in model.backbone_params]\n",
    "            other_params = [param for param in model.parameters() if id(param) not in backbone_ids]\n",
    "\n",
    "            model_params = [\n",
    "                {'params': other_params},\n",
    "                {'params': model.backbone_params, 'lr': cfg.TRAIN.BACKBONE_LR}\n",
    "            ]\n",
    "        else:\n",
    "            backbone_ids = [id(item) for item in model.backbone_params]\n",
    "            k_params = model.k_params_id\n",
    "            other_params = [param for param in model.parameters() if id(param) not in k_params and id(param) not in backbone_ids]\n",
    "\n",
    "            model_params = [\n",
    "                {'params': other_params},\n",
    "                {'params': model.backbone_params, 'lr': cfg.TRAIN.BACKBONE_LR}\n",
    "            ]\n",
    "            k_reg_params = model.k_params\n",
    "            optimizer_k = optim.Adam(k_reg_params, lr=cfg.TRAIN.K_LR)\n",
    "\n",
    "else:\n",
    "    model_params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.TRAIN.OPTIMIZER.lower() == 'sgd':\n",
    "    optimizer = optim.SGD(model_params, lr=cfg.TRAIN.LR, momentum=cfg.TRAIN.MOMENTUM, nesterov=True)\n",
    "elif cfg.TRAIN.OPTIMIZER.lower() == 'adam':\n",
    "    optimizer = optim.Adam(model_params, lr=cfg.TRAIN.LR)\n",
    "else:\n",
    "    raise ValueError('Unknown optimizer {}'.format(cfg.TRAIN.OPTIMIZER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38433abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.FP16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to enable FP16.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05385149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DataParallel(model, device_ids=cfg.GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(cfg.OUTPUT_PATH).exists():\n",
    "    Path(cfg.OUTPUT_PATH).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "tfboardwriter = SummaryWriter(logdir=str(Path(cfg.OUTPUT_PATH) / 'tensorboard' / 'training_{}'.format(now_time)))\n",
    "wb = xlwt.Workbook()\n",
    "wb.__save_path = str(Path(cfg.OUTPUT_PATH) / ('train_eval_result_' + now_time + '.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf76ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DupStdoutFileManager(str(Path(cfg.OUTPUT_PATH) / ('train_log_' + now_time + '.log'))) as _:\n",
    "        print_easydict(cfg)\n",
    "        model = train_eval_model(model, criterion, optimizer, optimizer_k, image_dataset, dataloader, tfboardwriter, benchmark,\n",
    "                                 num_epochs=cfg.TRAIN.NUM_EPOCHS,\n",
    "                                 start_epoch=cfg.TRAIN.START_EPOCH,\n",
    "                                 xls_wb=wb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77a02c92",
   "metadata": {
    "tags": []
   },
   "source": [
    "wb.save(wb.__save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
